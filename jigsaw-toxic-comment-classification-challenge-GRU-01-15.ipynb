{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\anchen.li\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from utils import *\n",
    "from __future__ import division, print_function\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "batch_size=64\n",
    "max_features = 20000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_embedding_list():\n",
    "    embedding_word_dict = {}\n",
    "    embedding_list = []\n",
    "    with open(\"wiki-news-300d-1M.vec\", encoding=\"utf8\") as f:\n",
    "        for row in tqdm.tqdm(f.read().split(\"\\n\")[1:-1]):\n",
    "            data = row.split(\" \")\n",
    "            word = data[0]\n",
    "            embedding = np.array([float(num) for num in data[1:-1]])\n",
    "            embedding_list.append(embedding)\n",
    "            embedding_word_dict[word] = len(embedding_word_dict)\n",
    "\n",
    "    embedding_list = np.array(embedding_list)\n",
    "    return embedding_list, embedding_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_trainData = pd.read_csv(path+'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences_train = _trainData[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_testData = pd.read_csv(path+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences_test = _testData[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=True, stem_words=True):\n",
    "    #Remove Special Characters\n",
    "    text=special_character_removal.sub('',text)\n",
    "    \n",
    "    #Replace Numbers\n",
    "    text=replace_numbers.sub('n',text)\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text_to_wordlist(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explanationwhi edit made usernam hardcor metallica fan revert werent vandal closur gas vote new york doll fac pleas dont remov templat talk page sinc im retir nown\n"
     ]
    }
   ],
   "source": [
    "print(comments[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True)\n",
    "# tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(comments + test_comments))\n",
    "comments_sequence = tokenizer.texts_to_sequences(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_labels_train = _trainData[list_classes].values\n",
    "test_comments_sequence = tokenizer.texts_to_sequences(test_comments)    \n",
    "_X_train = sequence.pad_sequences(comments_sequence , maxlen=maxlen)\n",
    "X_train = _X_train[0:159071,:]\n",
    "labels_train =_labels_train[0:159071,:]\n",
    "val_train = _X_train[159071:159572,:]\n",
    "val_labels_train =_labels_train[159071:159572,:]\n",
    "Test_train = sequence.pad_sequences(test_comments_sequence, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 100)\n"
     ]
    }
   ],
   "source": [
    "print(_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1000000/1000000 [02:14<00:00, 7455.76it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_list, embedding_word_dict = read_embedding_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0033470585653333162, 0.1098556062554554)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_mean,emb_std = embedding_list.mean(), embedding_list.std()\n",
    "emb_mean,emb_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words =len(word_index.items())\n",
    "embed_size= embedding_list.shape[1]\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words: continue\n",
    "    vec_idx = embedding_word_dict.get(word)\n",
    "    if vec_idx is not None:\n",
    "            embedding_vector = embedding_list[vec_idx]\n",
    "            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378392\n"
     ]
    }
   ],
   "source": [
    "print(len(word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "#Test_train = Test_train.reshape((Test_train.shape[0], 1, Test_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 100, 300)          113517600 \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 100, 200)          240600    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 113,780,106\n",
      "Trainable params: 113,779,506\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(nb_words, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(GRU(100, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(100, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.load_weights(\"toxic.hdf\")\n",
    "#model.fit(X_train, labels_train, batch_size=128, epochs=1)\n",
    "\n",
    "#y_test = model2.predict([X_te], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.fit(X_train, labels_train, batch_size=128, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.save_weights(\"model_GRU_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preds = model.predict(Test_train)\n",
    "#sample_submission = pd.read_csv(f'{path}sample_submission.csv')\n",
    "#sample_submission[list_classes] = preds\n",
    "#sample_submission.to_csv('submission_textgru_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159071 samples, validate on 500 samples\n",
      "Epoch 1/120\n",
      " - 5648s - loss: 0.0673 - acc: 0.9781 - val_loss: 0.0316 - val_acc: 0.9877\n",
      "Epoch 2/120\n",
      " - 5676s - loss: 0.0509 - acc: 0.9815 - val_loss: 0.0314 - val_acc: 0.9897\n",
      "Epoch 3/120\n",
      " - 5671s - loss: 0.0451 - acc: 0.9830 - val_loss: 0.0353 - val_acc: 0.9873\n",
      "Epoch 4/120\n",
      " - 5633s - loss: 0.0406 - acc: 0.9845 - val_loss: 0.0329 - val_acc: 0.9890\n",
      "Epoch 5/120\n",
      " - 5603s - loss: 0.0364 - acc: 0.9859 - val_loss: 0.0385 - val_acc: 0.9870\n",
      "Epoch 6/120\n",
      " - 5589s - loss: 0.0328 - acc: 0.9873 - val_loss: 0.0380 - val_acc: 0.9850\n",
      "Epoch 7/120\n",
      " - 5594s - loss: 0.0296 - acc: 0.9886 - val_loss: 0.0367 - val_acc: 0.9857\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "save_best = ModelCheckpoint('toxic.hdf', save_best_only=True, \n",
    "                           monitor='val_loss', mode='min')\n",
    "\n",
    "history = model.fit(X_train, labels_train,batch_size=64,validation_data=(val_train, val_labels_train),\n",
    "                    epochs=120, verbose=2,callbacks=[early_stopping,save_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.layers[0].trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.fit(X_train, labels_train, batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.save_weights(\"model_double_GRU.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list_sentences_test_1 =['We are using expect for the assertions. and React Test Utils with mocha-jsdom so that we can render out some React DOM and use standard javascript commands to test our react components.']\n",
    "#test_comments_1=[]\n",
    "#for text in list_sentences_test_1:\n",
    "#    test_comments_1.append(text_to_wordlist(text))\n",
    "    \n",
    "#test_comments_sequence_1 = tokenizer.texts_to_sequences(test_comments_1)   \n",
    "\n",
    "T#est_train_1 = sequence.pad_sequences(test_comments_sequence_1, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preds = model.predict(Test_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#print(np.argmax(preds, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(Test_train)\n",
    "sample_submission = pd.read_csv(f'{path}sample_submission.csv')\n",
    "sample_submission[list_classes] = preds\n",
    "sample_submission.to_csv('submission_textlstm_4.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
