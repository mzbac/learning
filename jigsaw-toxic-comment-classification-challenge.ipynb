{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from utils import *\n",
    "from __future__ import division, print_function\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "batch_size=64\n",
    "max_features = 20000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_trainData = pd.read_csv(path+'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences_train = _trainData[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "labels_train = _trainData[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_testData = pd.read_csv(path+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences_test = _testData[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=True, stem_words=True):\n",
    "    #Remove Special Characters\n",
    "    text=special_character_removal.sub('',text)\n",
    "    \n",
    "    #Replace Numbers\n",
    "    text=replace_numbers.sub('n',text)\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_glove_dataset(dataset):\n",
    "    \"\"\"Download the requested glove dataset from files.fast.ai\n",
    "    and return a location that can be passed to load_vectors.\n",
    "    \"\"\"\n",
    "    # see wordvectors.ipynb for info on how these files were\n",
    "    # generated from the original glove data.\n",
    "    md5sums = {'6B.50d': '8e1557d1228decbda7db6dfd81cd9909',\n",
    "               '6B.100d': 'c92dbbeacde2b0384a43014885a60b2c',\n",
    "               '6B.200d': 'af271b46c04b0b2e41a84d8cd806178d',\n",
    "               '6B.300d': '30290210376887dcc6d0a5a6374d8255'}\n",
    "    glove_path = os.path.abspath('data/glove/results')\n",
    "    %mkdir -p $glove_path\n",
    "    return get_file(dataset,\n",
    "                    'http://files.fast.ai/models/glove/' + dataset + '.tgz',\n",
    "                    cache_subdir=glove_path,\n",
    "                    md5_hash=md5sums.get(dataset, None),\n",
    "                    untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vectors(loc):\n",
    "    return (load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb'),encoding='latin1'),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb'),encoding='latin1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs, words, wordidx = load_vectors(get_glove_dataset('6B.50d'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text_to_wordlist(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comments[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True)\n",
    "# tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(comments + test_comments))\n",
    "comments_sequence = tokenizer.texts_to_sequences(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_comments_sequence = tokenizer.texts_to_sequences(test_comments)    \n",
    "X_train = sequence.pad_sequences(comments_sequence , maxlen=maxlen)\n",
    "Test_train = sequence.pad_sequences(test_comments_sequence, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_mean,emb_std = vecs.mean(), vecs.std()\n",
    "emb_mean,emb_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = vecs.shape[0]\n",
    "embed_size= vecs.shape[1]\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words: continue\n",
    "    vec_idx = wordidx.get(word)\n",
    "    if vec_idx is not None:\n",
    "            embedding_vector = vecs[vec_idx]\n",
    "            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "Test_train = Test_train.reshape((Test_train.shape[0], 1, Test_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_filters = 32\n",
    "\n",
    "inp = Input(shape=(1, maxlen,))\n",
    "x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "x1 = Conv2D(number_filters, (3, embed_size), data_format='channels_first',padding='same')(x)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Activation('relu')(x1)\n",
    "x1 = MaxPooling2D((int(int(x1.shape[2])  / 1.5), 1), data_format='channels_first')(x1)\n",
    "#x1 = Flatten()(x1)\n",
    "\n",
    "x2 = Conv2D(number_filters, (4, embed_size), data_format='channels_first',padding='same')(x)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Activation('elu')(x2)\n",
    "x2 = MaxPooling2D((int(int(x2.shape[2])  / 1.5), 1), data_format='channels_first')(x2)\n",
    "#x2 = Flatten()(x2)\n",
    "\n",
    "x3 = Conv2D(number_filters, (5, embed_size), data_format='channels_first',padding='same')(x)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Activation('relu')(x3)\n",
    "x3 = MaxPooling2D((int(int(x3.shape[2])  / 1.5), 1), data_format='channels_first')(x3)\n",
    "#x3 = Flatten()(x3)\n",
    "\n",
    "x4 = Conv2D(number_filters, (6, embed_size), data_format='channels_first',padding='same')(x)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Activation('elu')(x4)\n",
    "x4 = MaxPooling2D((int(int(x4.shape[2])  / 1.5), 1), data_format='channels_first')(x4)\n",
    "#x4 = Flatten()(x4)\n",
    "\n",
    "x5 = Conv2D(number_filters, (7, embed_size), data_format='channels_first',padding='same')(x)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = Activation('relu')(x5)\n",
    "x5 = MaxPooling2D((int(int(x5.shape[2])  / 1.5), 1), data_format='channels_first')(x5)\n",
    "#x5 = Flatten()(x5)\n",
    "\n",
    "# x6 = Conv2D(number_filters, (5, embed_size), data_format='channels_first')(x)\n",
    "# x6 = BatchNormalization()(x6)\n",
    "# x6 = Activation('elu')(x6)\n",
    "# x6 = MaxPooling2D((int(int(x6.shape[2])  / 1.5), 1), data_format='channels_first')(x6)\n",
    "# x6 = Flatten()(x6)\n",
    "\n",
    "# x7 = Conv2D(number_filters, (6, embed_size), data_format='channels_first')(x)\n",
    "# x7 = BatchNormalization()(x7)\n",
    "# x7 = Activation('relu')(x7)\n",
    "# x7 = MaxPooling2D((int(int(x7.shape[2])  / 1.5), 1), data_format='channels_first')(x7)\n",
    "# x7 = Flatten()(x7)\n",
    "\n",
    "# x8 = Conv2D(number_filters, (7, embed_size), data_format='channels_first')(x)\n",
    "# x8 = BatchNormalization()(x8)\n",
    "# x8 = Activation('elu')(x8)\n",
    "# x8 = MaxPooling2D((int(int(x8.shape[2])  / 1.5), 1), data_format='channels_first')(x8)\n",
    "# x8 = Flatten()(x8)\n",
    "\n",
    "# x9 = Conv2D(number_filters, (8, embed_size), data_format='channels_first')(x)\n",
    "# x9 = BatchNormalization()(x9)\n",
    "# x9 = Activation('relu')(x9)\n",
    "# x9 = MaxPooling2D((int(int(x9.shape[2])  / 1.5), 1), data_format='channels_first')(x9)\n",
    "# x9 = Flatten()(x9)\n",
    "\n",
    "# x10 = Conv2D(number_filters, (9, embed_size), data_format='channels_first')(x)\n",
    "# x10 = BatchNormalization()(x10)\n",
    "# x10 = Activation('elu')(x10)\n",
    "# x10 = MaxPooling2D((int(int(x10.shape[2])  / 1.5), 1), data_format='channels_first')(x10)\n",
    "# x10 = Flatten()(x10)\n",
    "\n",
    "x = merge([x1, x2, x3, x4, x5])\n",
    "x = BatchNormalization()(x)\n",
    "Conv2D(number_filters*2, (3, 3), data_format='channels_first',padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D((1, 1), data_format='channels_first')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(128, activation=\"elu\")(x)\n",
    "#x = Dropout(0.1)(x)\n",
    "# x = Dense(256, activation=\"relu\")(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"softmax\")(x)\n",
    "#x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model2 = Model(inputs=inp, outputs=x)\n",
    "#model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(X_train, labels_train, batch_size=64, epochs=1)\n",
    "\n",
    "#y_test = model2.predict([X_te], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model2.predict(Test_train)\n",
    "sample_submission = pd.read_csv(f'{path}sample_submission.csv')\n",
    "sample_submission[list_classes] = preds\n",
    "sample_submission.to_csv('submission_textcnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.save_weights(\"model_textcnn.h5\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
