{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "import math\n",
    "import six\n",
    "from sklearn.preprocessing import normalize\n",
    "from utils import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ASCII(s):\n",
    "    x = 0\n",
    "    for i in xrange(len(s)):\n",
    "        x += ord(s[i])*2**(8 * (len(s) - i - 1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trains = pd.read_csv(path+'train.csv', keep_default_na=False).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "_trains = np.transpose(trains)\n",
    "trains = _trains[:,0:791]\n",
    "val_trains = _trains[:,791:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 791)\n",
      "(12, 99)\n"
     ]
    }
   ],
   "source": [
    "print(trains.shape)\n",
    "print(val_trains.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(791,)\n",
      "(891,)\n"
     ]
    }
   ],
   "source": [
    "y_trains = _trains[1]\n",
    "_y_trains = np.transpose(y_trains)\n",
    "y_trains = _y_trains[0:791]\n",
    "y_val_trains =_y_trains[791:-1]\n",
    "print(y_trains.shape)\n",
    "print(_y_trains.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_trains= np.delete(trains,[1,3],axis=0)\n",
    "val_trains= np.delete(val_trains,[1,3],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 791)\n"
     ]
    }
   ],
   "source": [
    "print(x_trains.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'C' 'S' 'S' 'S' 'Q' 'S' 'S' 'S' 'C' 'S' 'S' 'S' 'S' 'S' 'S' 'Q' 'S' 'S' 'C' 'S' 'S' 'Q' 'S' 'S'\n",
      " 'S' 'C' 'S' 'Q' 'S' 'C' 'C' 'Q' 'S' 'C' 'S' 'C' 'S' 'S' 'C' 'S' 'S' 'C' 'C' 'Q' 'S' 'Q' 'Q' 'C' 'S'\n",
      " 'S' 'S' 'C' 'S' 'C' 'S' 'S' 'C' 'S' 'S' 'C' '' 'S' 'S' 'C' 'C' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'C' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'Q' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'C' 'C' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'Q' 'S' 'C' 'S' 'S' 'C' 'S' 'Q' 'S' 'C' 'S' 'S' 'S' 'C' 'S' 'S'\n",
      " 'C' 'Q' 'S' 'C' 'S' 'C' 'S' 'S' 'S' 'S' 'C' 'S' 'S' 'S' 'C' 'C' 'S' 'S' 'Q' 'S' 'S' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'C' 'Q' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'Q' 'S' 'S' 'C'\n",
      " 'S' 'S' 'C' 'S' 'S' 'S' 'C' 'S' 'S' 'S' 'S' 'Q' 'S' 'Q' 'S' 'S' 'S' 'S' 'S' 'C' 'C' 'Q' 'S' 'Q' 'S'\n",
      " 'S' 'S' 'S' 'C' 'S' 'S' 'S' 'C' 'Q' 'C' 'S' 'S' 'S' 'S' 'Q' 'C' 'S' 'S' 'C' 'S' 'S' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'C' 'Q' 'S' 'S' 'C' 'Q' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'C' 'C' 'S' 'C' 'S' 'Q' 'S' 'S' 'S' 'Q' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'C' 'Q'\n",
      " 'S' 'S' 'S' 'Q' 'S' 'Q' 'S' 'S' 'S' 'S' 'C' 'S' 'S' 'S' 'Q' 'S' 'C' 'C' 'S' 'S' 'C' 'C' 'S' 'S' 'C'\n",
      " 'Q' 'Q' 'S' 'Q' 'S' 'S' 'C' 'C' 'C' 'C' 'C' 'C' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'C' 'S' 'S' 'Q' 'S' 'S'\n",
      " 'C' 'S' 'S' 'S' 'C' 'Q' 'S' 'S' 'S' 'S' 'S' 'S' 'C' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'C' 'S' 'C' 'S' 'S' 'S' 'Q' 'Q' 'S' 'C' 'C' 'S' 'Q' 'S' 'C' 'C' 'Q' 'C' 'C' 'S' 'S' 'C' 'S'\n",
      " 'C' 'S' 'C' 'C' 'S' 'C' 'C' 'S' 'S' 'S' 'S' 'S' 'S' 'Q' 'C' 'S' 'S' 'S' 'C' 'S' 'S' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'Q' 'Q' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'C' 'Q' 'S' 'S' 'S'\n",
      " 'S' 'S' 'S' 'Q' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'C' 'S'\n",
      " 'S' 'S' 'C' 'C' 'S' 'C' 'S' 'S' 'S' 'Q' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'Q' 'C' 'S' 'S' 'S' 'C' 'S'\n",
      " 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'C' 'S' 'S' 'C' 'S' 'S' 'S' 'S' 'S' 'C' 'S' 'C' 'C' 'S' 'S' 'S'\n",
      " 'S' 'Q' 'Q' 'S' 'S' 'C' 'S' 'S' 'S' 'S' 'Q' 'S' 'S' 'C' 'S' 'S' 'S' 'Q' 'S' 'S' 'S' 'S' 'C' 'C' 'C'\n",
      " 'Q' 'S' 'S' 'S' 'S' 'S' 'C' 'C' 'C' 'S' 'S' 'S' 'C' 'S' 'C' 'S' 'S' 'S' 'S' 'C' 'S' 'S' 'C' 'S' 'S'\n",
      " 'C' 'S' 'Q' 'C' 'S' 'S' 'C' 'C' 'S' 'S' 'Q' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'C' 'S' 'S' 'S' 'S' 'Q' 'S'\n",
      " 'S' 'S' 'S' 'C' 'S' 'S' 'C' 'S' 'C' 'C' 'S' 'S' 'C' 'S' 'S' 'S' 'C' 'S' 'Q' 'S' 'S' 'S' 'S' 'C' 'C'\n",
      " 'S' 'S' 'S' 'S' 'C' 'S' 'S' 'S' 'C' 'S' 'S' 'S' 'Q' 'Q' 'S' 'S' 'S' 'S' 'S' 'S' 'C' 'S' 'C' 'S' 'S'\n",
      " 'S' 'Q' 'S' 'S' 'Q' 'S' 'S' 'C' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'C' 'S' 'S' 'C' 'C' 'S' 'C' 'S' 'S'\n",
      " 'S' 'S' 'S' 'Q' 'Q' 'S' 'S' 'Q' 'S' 'C' 'S' 'C' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'S' 'S' 'C' 'Q' 'C' 'S' 'S' 'S' 'C' 'S' 'S' 'S' 'S' 'S' 'C' 'S' 'C' 'S' 'S' 'S' 'Q' 'C' 'S'\n",
      " 'C' 'S' 'C' 'Q' 'S' 'S' 'S' 'S' 'S' 'C' 'C' 'S' 'S' 'S' 'S' 'S' 'C' 'S' 'Q' 'S' 'S' 'S' 'S' 'S' 'S'\n",
      " 'S' 'S' 'Q' 'S' 'S' 'S' 'C' 'S' 'S' 'S' 'S' 'S' 'C' 'S' 'S' 'S' 'S' 'C' 'S' 'S' 'S' 'S' 'S' 'S' 'Q'\n",
      " 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'S' 'C' 'S' 'S' 'S' 'C' 'Q' 'Q' 'S' 'S' 'S' 'S' 'C' 'S'\n",
      " 'S' 'Q' 'S' 'Q' 'S' 'C' 'S' 'S' 'S' 'S' 'S' 'S' 'Q' 'S' 'C' 'Q']\n"
     ]
    }
   ],
   "source": [
    "print(x_trains[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_trains[2] = [ASCII(c) for c in x_trains[2]]\n",
    "x_trains[3] = [ASCII(c) for c in x_trains[3]]\n",
    "x_trains[6] = [ASCII(c) for c in x_trains[6]]\n",
    "\n",
    "val_trains[2] = [ASCII(c) for c in val_trains[2]]\n",
    "val_trains[3] = [ASCII(c) for c in val_trains[3]]\n",
    "val_trains[6] = [ASCII(c) for c in val_trains[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_trains[8] = [ASCII(str(c)) for c in x_trains[8]]\n",
    "x_trains[9] = [ASCII(str(c)) for c in x_trains[9]]\n",
    "\n",
    "val_trains[8] = [ASCII(str(c)) for c in val_trains[8]]\n",
    "val_trains[9] = [ASCII(str(c)) for c in val_trains[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_px = x_trains.mean()\n",
    "std_px = x_trains.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_input(x): \n",
    "    return (x-mean_px)/std_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "x_trains=normalize(x_trains, axis=1, norm='l1')\n",
    "val_trains=normalize(val_trains, axis=1, norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model_bn_do():\n",
    "    model = Sequential([\n",
    "    Dense(32,input_shape=(10,), activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(2048, activation='relu'),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.optimizer.lr=1e-5\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_8 (Dense)                  (None, 32)            352         dense_input_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 64)            2112        dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 128)           8320        dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 256)           33024       dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 512)           131584      dense_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 1024)          525312      dense_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 2048)          2099200     dense_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                 (None, 1024)          2098176     dense_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 512)           524800      dense_15[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_17 (Dense)                 (None, 1)             513         dense_16[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 5423393\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = get_model_bn_do()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.25 71.2833 7.925 53.1 8.05 8.4583 51.8625 21.075 11.1333 30.0708 16.7 26.55 8.05 31.275 7.8542\n",
      " 16.0 29.125 13.0 18.0 7.225 26.0 13.0 8.0292 35.5 21.075 31.3875 7.225 263.0 7.8792 7.8958 27.7208\n",
      " 146.5208 7.75 10.5 82.1708 52.0 7.2292 8.05 18.0 11.2417 9.475 21.0 7.8958 41.5792 7.8792 8.05 15.5\n",
      " 7.75 21.6792 17.8 39.6875 7.8 76.7292 26.0 61.9792 35.5 10.5 7.2292 27.75 46.9 7.2292 80.0 83.475\n",
      " 27.9 27.7208 15.2458 10.5 8.1583 7.925 8.6625 10.5 46.9 73.5 14.4542 56.4958 7.65 7.8958 8.05 29.0\n",
      " 12.475 9.0 9.5 7.7875 47.1 10.5 15.85 34.375 8.05 263.0 8.05 8.05 7.8542 61.175 20.575 7.25 8.05\n",
      " 34.6542 63.3583 23.0 26.0 7.8958 7.8958 77.2875 8.6542 7.925 7.8958 7.65 7.775 7.8958 24.15 52.0\n",
      " 14.4542 8.05 9.825 14.4583 7.925 7.75 21.0 247.5208 31.275 73.5 8.05 30.0708 13.0 77.2875 11.2417\n",
      " 7.75 7.1417 22.3583 6.975 7.8958 7.05 14.5 26.0 13.0 15.0458 26.2833 53.1 9.2167 79.2 15.2458 7.75\n",
      " 15.85 6.75 11.5 36.75 7.7958 34.375 26.0 13.0 12.525 66.6 8.05 14.5 7.3125 61.3792 7.7333 8.05\n",
      " 8.6625 69.55 16.1 15.75 7.775 8.6625 39.6875 20.525 55.0 27.9 25.925 56.4958 33.5 29.125 11.1333\n",
      " 7.925 30.6958 7.8542 25.4667 28.7125 13.0 0.0 69.55 15.05 31.3875 39.0 22.025 50.0 15.5 26.55 15.5\n",
      " 7.8958 13.0 13.0 7.8542 26.0 27.7208 146.5208 7.75 8.4042 7.75 13.0 9.5 69.55 6.4958 7.225 8.05\n",
      " 10.4625 15.85 18.7875 7.75 31.0 7.05 21.0 7.25 13.0 7.75 113.275 7.925 27.0 76.2917 10.5 8.05 13.0\n",
      " 8.05 7.8958 90.0 9.35 10.5 7.25 13.0 25.4667 83.475 7.775 13.5 31.3875 10.5 7.55 26.0 26.25 10.5\n",
      " 12.275 14.4542 15.5 10.5 7.125 7.225 90.0 7.775 14.5 52.5542 26.0 7.25 10.4625 26.55 16.1 20.2125\n",
      " 15.2458 79.2 86.5 512.3292 26.0 7.75 31.3875 79.65 0.0 7.75 10.5 39.6875 7.775 153.4625 135.6333\n",
      " 31.0 0.0 19.5 29.7 7.75 77.9583 7.75 0.0 29.125 20.25 7.75 7.8542 9.5 8.05 26.0 8.6625 9.5 7.8958\n",
      " 13.0 7.75 78.85 91.0792 12.875 8.85 7.8958 27.7208 7.2292 151.55 30.5 247.5208 7.75 23.25 0.0 12.35\n",
      " 8.05 151.55 110.8833 108.9 24.0 56.9292 83.1583 262.375 26.0 7.8958 26.25 7.8542 26.0 14.0 164.8667\n",
      " 134.5 7.25 7.8958 12.35 29.0 69.55 135.6333 6.2375 13.0 20.525 57.9792 23.25 28.5 153.4625 18.0\n",
      " 133.65 7.8958 66.6 134.5 8.05 35.5 26.0 263.0 13.0 13.0 13.0 13.0 13.0 16.1 15.9 8.6625 9.225 35.0\n",
      " 7.2292 17.8 7.225 9.5 55.0 13.0 7.8792 7.8792 27.9 27.7208 14.4542 7.05 15.5 7.25 75.25 7.2292 7.75\n",
      " 69.3 55.4417 6.4958 8.05 135.6333 21.075 82.1708 7.25 211.5 4.0125 7.775 227.525 15.7417 7.925 52.0\n",
      " 7.8958 73.5 46.9 13.0 7.7292 12.0 120.0 7.7958 7.925 113.275 16.7 7.7958 7.8542 26.0 10.5 12.65\n",
      " 7.925 8.05 9.825 15.85 8.6625 21.0 7.75 18.75 7.775 25.4667 7.8958 6.8583 90.0 0.0 7.925 8.05 32.5\n",
      " 13.0 13.0 24.15 7.8958 7.7333 7.875 14.4 20.2125 7.25 26.0 26.0 7.75 8.05 26.55 16.1 26.0 7.125\n",
      " 55.9 120.0 34.375 18.75 263.0 10.5 26.25 9.5 7.775 13.0 8.1125 81.8583 19.5 26.55 19.2583 30.5\n",
      " 27.75 19.9667 27.75 89.1042 8.05 7.8958 26.55 51.8625 10.5 7.75 26.55 8.05 38.5 13.0 8.05 7.05 0.0\n",
      " 26.55 7.725 19.2583 7.25 8.6625 27.75 13.7917 9.8375 52.0 21.0 7.0458 7.5208 12.2875 46.9 0.0 8.05\n",
      " 9.5875 91.0792 25.4667 90.0 29.7 8.05 15.9 19.9667 7.25 30.5 49.5042 8.05 14.4583 78.2667 15.1\n",
      " 151.55 7.7958 8.6625 7.75 7.6292 9.5875 86.5 108.9 26.0 26.55 22.525 56.4958 7.75 8.05 26.2875 59.4\n",
      " 7.4958 34.0208 10.5 24.15 26.0 7.8958 93.5 7.8958 7.225 57.9792 7.2292 7.75 10.5 221.7792 7.925\n",
      " 11.5 26.0 7.2292 7.2292 22.3583 8.6625 26.25 26.55 106.425 14.5 49.5 71.0 31.275 31.275 26.0\n",
      " 106.425 26.0 26.0 13.8625 20.525 36.75 110.8833 26.0 7.8292 7.225 7.775 26.55 39.6 227.525 79.65\n",
      " 17.4 7.75 7.8958 13.5 8.05 8.05 24.15 7.8958 21.075 7.2292 7.8542 10.5 51.4792 26.3875 7.75 8.05\n",
      " 14.5 13.0 55.9 14.4583 7.925 30.0 110.8833 26.0 40.125 8.7125 79.65 15.0 79.2 8.05 8.05 7.125\n",
      " 78.2667 7.25 7.75 26.0 24.15 33.0 0.0 7.225 56.9292 27.0 7.8958 42.4 8.05 26.55 15.55 7.8958 30.5\n",
      " 41.5792 153.4625 31.275 7.05 15.5 7.75 8.05 65.0 14.4 16.1 39.0 10.5 14.4542 52.5542 15.7417 7.8542\n",
      " 16.1 32.3208 12.35 77.9583 7.8958 7.7333 30.0 7.0542 30.5 0.0 27.9 13.0 7.925 26.25 39.6875 16.1\n",
      " 7.8542 69.3 27.9 56.4958 19.2583 76.7292 7.8958 35.5 7.55 7.55 7.8958 23.0 8.4333 7.8292 6.75 73.5\n",
      " 7.8958 15.5 13.0 113.275 133.65 7.225 25.5875 7.4958 7.925 73.5 13.0 7.775 8.05 52.0 39.0 52.0 10.5\n",
      " 13.0 0.0 7.775 8.05 9.8417 46.9 512.3292 8.1375 76.7292 9.225 46.9 39.0 41.5792 39.6875 10.1708\n",
      " 7.7958 211.3375 57.0 13.4167 56.4958 7.225 26.55 13.5 8.05 7.7333 110.8833 7.65 227.525 26.2875\n",
      " 14.4542 7.7417 7.8542 26.0 13.5 26.2875 151.55 15.2458 49.5042 26.55 52.0 9.4833 13.0 7.65 227.525\n",
      " 10.5 15.5 7.775 33.0 7.0542 13.0 13.0 53.1 8.6625 21.0 7.7375 26.0 7.925 211.3375 18.7875 0.0 13.0\n",
      " 13.0 16.1 34.375 512.3292 7.8958 7.8958 30.0 78.85 262.375 16.1 7.925 71.0 20.25 13.0 53.1 7.75\n",
      " 23.0 12.475 9.5 7.8958 65.0 14.5 7.7958 11.5 8.05 86.5 14.5 7.125 7.2292 120.0 7.775 77.9583 39.6\n",
      " 7.75 24.15 8.3625 9.5 7.8542 10.5 7.225 23.0 7.75 7.75 12.475 7.7375 211.3375 7.2292 57.0 30.0\n",
      " 23.45 7.05 7.25 7.4958 29.125 20.575 79.2 7.75]\n"
     ]
    }
   ],
   "source": [
    "print(x_trains[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_trains = pd.read_csv(path+'test.csv', keep_default_na=False).values\n",
    "test_trains=np.transpose(test_trains)\n",
    "test_trains= np.delete(test_trains,2,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "test_trains[2] = [ASCII(c) for c in test_trains[2]]\n",
    "test_trains[3] = [ASCII(c) for c in test_trains[3]]\n",
    "test_trains[6] = [ASCII(c) for c in test_trains[6]]\n",
    "test_trains[7] = [ASCII(str(c)) for c in test_trains[7]]\n",
    "test_trains[8] = [ASCII(str(c)) for c in test_trains[8]]\n",
    "test_trains[9] = [ASCII(str(c)) for c in test_trains[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "test_trains=normalize(test_trains, axis=1, norm='l1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_trains' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f68dbdce37e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_trains\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_trains' is not defined"
     ]
    }
   ],
   "source": [
    "print(test_trains[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 791 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "791/791 [==============================] - 1s - loss: 0.4562 - acc: 0.8028 - val_loss: 0.6666 - val_acc: 0.7677\n",
      "Epoch 2/100\n",
      "791/791 [==============================] - 1s - loss: 0.4540 - acc: 0.7990 - val_loss: 0.6741 - val_acc: 0.7677\n",
      "Epoch 3/100\n",
      "791/791 [==============================] - 1s - loss: 0.4566 - acc: 0.8015 - val_loss: 0.6653 - val_acc: 0.7778\n",
      "Epoch 4/100\n",
      "791/791 [==============================] - 1s - loss: 0.4563 - acc: 0.7977 - val_loss: 0.6750 - val_acc: 0.7677\n",
      "Epoch 5/100\n",
      "791/791 [==============================] - 1s - loss: 0.4532 - acc: 0.8053 - val_loss: 0.6670 - val_acc: 0.7677\n",
      "Epoch 6/100\n",
      "791/791 [==============================] - 1s - loss: 0.4705 - acc: 0.7965 - val_loss: 0.6822 - val_acc: 0.7677\n",
      "Epoch 7/100\n",
      "791/791 [==============================] - 1s - loss: 0.4533 - acc: 0.7952 - val_loss: 0.6742 - val_acc: 0.7677\n",
      "Epoch 8/100\n",
      "791/791 [==============================] - 1s - loss: 0.4546 - acc: 0.8015 - val_loss: 0.6690 - val_acc: 0.7677\n",
      "Epoch 9/100\n",
      "791/791 [==============================] - 1s - loss: 0.4674 - acc: 0.8003 - val_loss: 0.6930 - val_acc: 0.7677\n",
      "Epoch 10/100\n",
      "791/791 [==============================] - 1s - loss: 0.4585 - acc: 0.7977 - val_loss: 0.6724 - val_acc: 0.7778\n",
      "Epoch 11/100\n",
      "791/791 [==============================] - 1s - loss: 0.4547 - acc: 0.7952 - val_loss: 0.6869 - val_acc: 0.7677\n",
      "Epoch 12/100\n",
      "791/791 [==============================] - 1s - loss: 0.4539 - acc: 0.8015 - val_loss: 0.6790 - val_acc: 0.7677\n",
      "Epoch 13/100\n",
      "791/791 [==============================] - 1s - loss: 0.4560 - acc: 0.7914 - val_loss: 0.6855 - val_acc: 0.7677\n",
      "Epoch 14/100\n",
      "791/791 [==============================] - 1s - loss: 0.4537 - acc: 0.8003 - val_loss: 0.6806 - val_acc: 0.7677\n",
      "Epoch 15/100\n",
      "791/791 [==============================] - 1s - loss: 0.4554 - acc: 0.8003 - val_loss: 0.6847 - val_acc: 0.7677\n",
      "Epoch 16/100\n",
      "791/791 [==============================] - 1s - loss: 0.4522 - acc: 0.7952 - val_loss: 0.6904 - val_acc: 0.7677\n",
      "Epoch 17/100\n",
      "791/791 [==============================] - 1s - loss: 0.4553 - acc: 0.8015 - val_loss: 0.6855 - val_acc: 0.7677\n",
      "Epoch 18/100\n",
      "791/791 [==============================] - 1s - loss: 0.4519 - acc: 0.8028 - val_loss: 0.6930 - val_acc: 0.7677\n",
      "Epoch 19/100\n",
      "791/791 [==============================] - 1s - loss: 0.4534 - acc: 0.7990 - val_loss: 0.6951 - val_acc: 0.7677\n",
      "Epoch 20/100\n",
      "791/791 [==============================] - 1s - loss: 0.4540 - acc: 0.8015 - val_loss: 0.6927 - val_acc: 0.7677\n",
      "Epoch 21/100\n",
      "791/791 [==============================] - 1s - loss: 0.4526 - acc: 0.7952 - val_loss: 0.7018 - val_acc: 0.7677\n",
      "Epoch 22/100\n",
      "791/791 [==============================] - 1s - loss: 0.4580 - acc: 0.8003 - val_loss: 0.6991 - val_acc: 0.7677\n",
      "Epoch 23/100\n",
      "791/791 [==============================] - 1s - loss: 0.4536 - acc: 0.8040 - val_loss: 0.6906 - val_acc: 0.7677\n",
      "Epoch 24/100\n",
      "791/791 [==============================] - 1s - loss: 0.4557 - acc: 0.7965 - val_loss: 0.6936 - val_acc: 0.7677\n",
      "Epoch 25/100\n",
      "791/791 [==============================] - 1s - loss: 0.4521 - acc: 0.7977 - val_loss: 0.6943 - val_acc: 0.7677\n",
      "Epoch 26/100\n",
      "791/791 [==============================] - 1s - loss: 0.4633 - acc: 0.8040 - val_loss: 0.6983 - val_acc: 0.7677\n",
      "Epoch 27/100\n",
      "791/791 [==============================] - 1s - loss: 0.4550 - acc: 0.7952 - val_loss: 0.7065 - val_acc: 0.7677\n",
      "Epoch 28/100\n",
      "791/791 [==============================] - 1s - loss: 0.4602 - acc: 0.7965 - val_loss: 0.6979 - val_acc: 0.7677\n",
      "Epoch 29/100\n",
      "791/791 [==============================] - 1s - loss: 0.4564 - acc: 0.8040 - val_loss: 0.6921 - val_acc: 0.7677\n",
      "Epoch 30/100\n",
      "791/791 [==============================] - 1s - loss: 0.4542 - acc: 0.8003 - val_loss: 0.6995 - val_acc: 0.7677\n",
      "Epoch 31/100\n",
      "791/791 [==============================] - 1s - loss: 0.4524 - acc: 0.7990 - val_loss: 0.7036 - val_acc: 0.7677\n",
      "Epoch 32/100\n",
      "791/791 [==============================] - 1s - loss: 0.4514 - acc: 0.7977 - val_loss: 0.7054 - val_acc: 0.7677\n",
      "Epoch 33/100\n",
      "791/791 [==============================] - 1s - loss: 0.4620 - acc: 0.7914 - val_loss: 0.7095 - val_acc: 0.7677\n",
      "Epoch 34/100\n",
      "791/791 [==============================] - 1s - loss: 0.4649 - acc: 0.8053 - val_loss: 0.6969 - val_acc: 0.7677\n",
      "Epoch 35/100\n",
      "791/791 [==============================] - 1s - loss: 0.4590 - acc: 0.7952 - val_loss: 0.7115 - val_acc: 0.7677\n",
      "Epoch 36/100\n",
      "791/791 [==============================] - 1s - loss: 0.4542 - acc: 0.7977 - val_loss: 0.7055 - val_acc: 0.7677\n",
      "Epoch 37/100\n",
      "791/791 [==============================] - 1s - loss: 0.4499 - acc: 0.8015 - val_loss: 0.7012 - val_acc: 0.7677\n",
      "Epoch 38/100\n",
      "791/791 [==============================] - 1s - loss: 0.4503 - acc: 0.7965 - val_loss: 0.7045 - val_acc: 0.7677\n",
      "Epoch 39/100\n",
      "791/791 [==============================] - 1s - loss: 0.4522 - acc: 0.7990 - val_loss: 0.7142 - val_acc: 0.7677\n",
      "Epoch 40/100\n",
      "791/791 [==============================] - 1s - loss: 0.4545 - acc: 0.7965 - val_loss: 0.7145 - val_acc: 0.7677\n",
      "Epoch 41/100\n",
      "791/791 [==============================] - 1s - loss: 0.4545 - acc: 0.8040 - val_loss: 0.7158 - val_acc: 0.7677\n",
      "Epoch 42/100\n",
      "791/791 [==============================] - 1s - loss: 0.4545 - acc: 0.8040 - val_loss: 0.7054 - val_acc: 0.7677\n",
      "Epoch 43/100\n",
      "791/791 [==============================] - 1s - loss: 0.4525 - acc: 0.7977 - val_loss: 0.7063 - val_acc: 0.7677\n",
      "Epoch 44/100\n",
      "791/791 [==============================] - 1s - loss: 0.4500 - acc: 0.7965 - val_loss: 0.7189 - val_acc: 0.7677\n",
      "Epoch 45/100\n",
      "791/791 [==============================] - 1s - loss: 0.4523 - acc: 0.7990 - val_loss: 0.7104 - val_acc: 0.7677\n",
      "Epoch 46/100\n",
      "791/791 [==============================] - 1s - loss: 0.4528 - acc: 0.8015 - val_loss: 0.7047 - val_acc: 0.7677\n",
      "Epoch 47/100\n",
      "791/791 [==============================] - 1s - loss: 0.4572 - acc: 0.7990 - val_loss: 0.7111 - val_acc: 0.7677\n",
      "Epoch 48/100\n",
      "791/791 [==============================] - 1s - loss: 0.4504 - acc: 0.7990 - val_loss: 0.7203 - val_acc: 0.7677\n",
      "Epoch 49/100\n",
      "791/791 [==============================] - 1s - loss: 0.4596 - acc: 0.8040 - val_loss: 0.7057 - val_acc: 0.7677\n",
      "Epoch 50/100\n",
      "791/791 [==============================] - 1s - loss: 0.4667 - acc: 0.8015 - val_loss: 0.7079 - val_acc: 0.7677\n",
      "Epoch 51/100\n",
      "791/791 [==============================] - 1s - loss: 0.4631 - acc: 0.7927 - val_loss: 0.7248 - val_acc: 0.7677\n",
      "Epoch 52/100\n",
      "791/791 [==============================] - 1s - loss: 0.4509 - acc: 0.8053 - val_loss: 0.7094 - val_acc: 0.7677\n",
      "Epoch 53/100\n",
      "791/791 [==============================] - 1s - loss: 0.4505 - acc: 0.8003 - val_loss: 0.7164 - val_acc: 0.7677\n",
      "Epoch 54/100\n",
      "791/791 [==============================] - 1s - loss: 0.4497 - acc: 0.7990 - val_loss: 0.7248 - val_acc: 0.7677\n",
      "Epoch 55/100\n",
      "791/791 [==============================] - 1s - loss: 0.4503 - acc: 0.7990 - val_loss: 0.7177 - val_acc: 0.7677\n",
      "Epoch 56/100\n",
      "791/791 [==============================] - 1s - loss: 0.4493 - acc: 0.7914 - val_loss: 0.7282 - val_acc: 0.7677\n",
      "Epoch 57/100\n",
      "791/791 [==============================] - 1s - loss: 0.4723 - acc: 0.7965 - val_loss: 0.7084 - val_acc: 0.7677\n",
      "Epoch 58/100\n",
      "791/791 [==============================] - 1s - loss: 0.4565 - acc: 0.7990 - val_loss: 0.7195 - val_acc: 0.7677\n",
      "Epoch 59/100\n",
      "791/791 [==============================] - 1s - loss: 0.4554 - acc: 0.7889 - val_loss: 0.7211 - val_acc: 0.7677\n",
      "Epoch 60/100\n",
      "791/791 [==============================] - 1s - loss: 0.4529 - acc: 0.7977 - val_loss: 0.7178 - val_acc: 0.7677\n",
      "Epoch 61/100\n",
      "791/791 [==============================] - 1s - loss: 0.4509 - acc: 0.8003 - val_loss: 0.7108 - val_acc: 0.7677\n",
      "Epoch 62/100\n",
      "791/791 [==============================] - 1s - loss: 0.4532 - acc: 0.8015 - val_loss: 0.7177 - val_acc: 0.7677\n",
      "Epoch 63/100\n",
      "791/791 [==============================] - 1s - loss: 0.4492 - acc: 0.7977 - val_loss: 0.7275 - val_acc: 0.7677\n",
      "Epoch 64/100\n",
      "791/791 [==============================] - 1s - loss: 0.4508 - acc: 0.7990 - val_loss: 0.7183 - val_acc: 0.7677\n",
      "Epoch 65/100\n",
      "791/791 [==============================] - 1s - loss: 0.4483 - acc: 0.7990 - val_loss: 0.7245 - val_acc: 0.7677\n",
      "Epoch 66/100\n",
      "791/791 [==============================] - 1s - loss: 0.4525 - acc: 0.7990 - val_loss: 0.7299 - val_acc: 0.7677\n",
      "Epoch 67/100\n",
      "791/791 [==============================] - 1s - loss: 0.4608 - acc: 0.8053 - val_loss: 0.7158 - val_acc: 0.7677\n",
      "Epoch 68/100\n",
      "791/791 [==============================] - 1s - loss: 0.4524 - acc: 0.8091 - val_loss: 0.7303 - val_acc: 0.7677\n",
      "Epoch 69/100\n",
      "791/791 [==============================] - 1s - loss: 0.4508 - acc: 0.7965 - val_loss: 0.7203 - val_acc: 0.7677\n",
      "Epoch 70/100\n",
      "791/791 [==============================] - 1s - loss: 0.4497 - acc: 0.7990 - val_loss: 0.7257 - val_acc: 0.7677\n",
      "Epoch 71/100\n",
      "791/791 [==============================] - 1s - loss: 0.4484 - acc: 0.8003 - val_loss: 0.7252 - val_acc: 0.7677\n",
      "Epoch 72/100\n",
      "791/791 [==============================] - 1s - loss: 0.4518 - acc: 0.7977 - val_loss: 0.7252 - val_acc: 0.7677\n",
      "Epoch 73/100\n",
      "791/791 [==============================] - 1s - loss: 0.4491 - acc: 0.8028 - val_loss: 0.7308 - val_acc: 0.7677\n",
      "Epoch 74/100\n",
      "791/791 [==============================] - 1s - loss: 0.4509 - acc: 0.7990 - val_loss: 0.7336 - val_acc: 0.7677\n",
      "Epoch 75/100\n",
      "791/791 [==============================] - 1s - loss: 0.4498 - acc: 0.8003 - val_loss: 0.7267 - val_acc: 0.7677\n",
      "Epoch 76/100\n",
      "791/791 [==============================] - 1s - loss: 0.4470 - acc: 0.8003 - val_loss: 0.7257 - val_acc: 0.7677\n",
      "Epoch 77/100\n",
      "791/791 [==============================] - 1s - loss: 0.4508 - acc: 0.7990 - val_loss: 0.7250 - val_acc: 0.7677\n",
      "Epoch 78/100\n",
      "791/791 [==============================] - 1s - loss: 0.4608 - acc: 0.7990 - val_loss: 0.7434 - val_acc: 0.7677\n",
      "Epoch 79/100\n",
      "791/791 [==============================] - 1s - loss: 0.4667 - acc: 0.7977 - val_loss: 0.7171 - val_acc: 0.7677\n",
      "Epoch 80/100\n",
      "791/791 [==============================] - 1s - loss: 0.4573 - acc: 0.8028 - val_loss: 0.7310 - val_acc: 0.7677\n",
      "Epoch 81/100\n",
      "791/791 [==============================] - 1s - loss: 0.4501 - acc: 0.8003 - val_loss: 0.7306 - val_acc: 0.7677\n",
      "Epoch 82/100\n",
      "791/791 [==============================] - 1s - loss: 0.4515 - acc: 0.7977 - val_loss: 0.7322 - val_acc: 0.7677\n",
      "Epoch 83/100\n",
      "791/791 [==============================] - 1s - loss: 0.4453 - acc: 0.8003 - val_loss: 0.7248 - val_acc: 0.7677\n",
      "Epoch 84/100\n",
      "791/791 [==============================] - 1s - loss: 0.4486 - acc: 0.7977 - val_loss: 0.7397 - val_acc: 0.7677\n",
      "Epoch 85/100\n",
      "791/791 [==============================] - 1s - loss: 0.4531 - acc: 0.8003 - val_loss: 0.7267 - val_acc: 0.7677\n",
      "Epoch 86/100\n",
      "791/791 [==============================] - 1s - loss: 0.4493 - acc: 0.8015 - val_loss: 0.7311 - val_acc: 0.7677\n",
      "Epoch 87/100\n",
      "791/791 [==============================] - 1s - loss: 0.4529 - acc: 0.8040 - val_loss: 0.7374 - val_acc: 0.7677\n",
      "Epoch 88/100\n",
      "791/791 [==============================] - 1s - loss: 0.4468 - acc: 0.7977 - val_loss: 0.7302 - val_acc: 0.7677\n",
      "Epoch 89/100\n",
      "791/791 [==============================] - 1s - loss: 0.4488 - acc: 0.7990 - val_loss: 0.7318 - val_acc: 0.7677\n",
      "Epoch 90/100\n",
      "791/791 [==============================] - 1s - loss: 0.4474 - acc: 0.7965 - val_loss: 0.7324 - val_acc: 0.7677\n",
      "Epoch 91/100\n",
      "791/791 [==============================] - 1s - loss: 0.4477 - acc: 0.8003 - val_loss: 0.7318 - val_acc: 0.7677\n",
      "Epoch 92/100\n",
      "791/791 [==============================] - 1s - loss: 0.4527 - acc: 0.7952 - val_loss: 0.7483 - val_acc: 0.7677\n",
      "Epoch 93/100\n",
      "791/791 [==============================] - 1s - loss: 0.4488 - acc: 0.8066 - val_loss: 0.7317 - val_acc: 0.7677\n",
      "Epoch 94/100\n",
      "791/791 [==============================] - 1s - loss: 0.4474 - acc: 0.7990 - val_loss: 0.7446 - val_acc: 0.7677\n",
      "Epoch 95/100\n",
      "791/791 [==============================] - 1s - loss: 0.4547 - acc: 0.7990 - val_loss: 0.7488 - val_acc: 0.7677\n",
      "Epoch 96/100\n",
      "791/791 [==============================] - 1s - loss: 0.4490 - acc: 0.8003 - val_loss: 0.7424 - val_acc: 0.7677\n",
      "Epoch 97/100\n",
      "791/791 [==============================] - 1s - loss: 0.4544 - acc: 0.7990 - val_loss: 0.7367 - val_acc: 0.7677\n",
      "Epoch 98/100\n",
      "791/791 [==============================] - 1s - loss: 0.4485 - acc: 0.8003 - val_loss: 0.7385 - val_acc: 0.7677\n",
      "Epoch 99/100\n",
      "791/791 [==============================] - 1s - loss: 0.4481 - acc: 0.7977 - val_loss: 0.7431 - val_acc: 0.7677\n",
      "Epoch 100/100\n",
      "791/791 [==============================] - 1s - loss: 0.4457 - acc: 0.8053 - val_loss: 0.7402 - val_acc: 0.7677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d39b5f150>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(np.transpose(x_trains), y_trains,validation_data=(np.transpose(val_trains), y_val_trains), nb_epoch=100, batch_size=64)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = model.predict(np.transpose(test_trains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1537]\n",
      " [ 0.7807]\n",
      " [ 0.1058]\n",
      " [ 0.0708]\n",
      " [ 0.7653]\n",
      " [ 0.0708]\n",
      " [ 0.9128]\n",
      " [ 0.0917]\n",
      " [ 0.9146]\n",
      " [ 0.022 ]\n",
      " [ 0.0708]\n",
      " [ 0.1688]\n",
      " [ 0.9235]\n",
      " [ 0.0589]\n",
      " [ 0.9139]\n",
      " [ 0.8752]\n",
      " [ 0.1055]\n",
      " [ 0.0682]\n",
      " [ 0.7802]\n",
      " [ 0.9144]\n",
      " [ 0.0907]\n",
      " [ 0.0985]\n",
      " [ 0.9437]\n",
      " [ 0.2392]\n",
      " [ 0.9991]\n",
      " [ 0.0406]\n",
      " [ 0.9351]\n",
      " [ 0.1601]\n",
      " [ 0.1685]\n",
      " [ 0.0238]\n",
      " [ 0.059 ]\n",
      " [ 0.026 ]\n",
      " [ 0.6071]\n",
      " [ 0.6071]\n",
      " [ 0.0908]\n",
      " [ 0.1702]\n",
      " [ 0.9122]\n",
      " [ 0.9122]\n",
      " [ 0.0704]\n",
      " [ 0.0769]\n",
      " [ 0.107 ]\n",
      " [ 0.1682]\n",
      " [ 0.002 ]\n",
      " [ 0.9352]\n",
      " [ 0.9227]\n",
      " [ 0.0711]\n",
      " [ 0.161 ]\n",
      " [ 0.07  ]\n",
      " [ 0.9439]\n",
      " [ 0.7849]\n",
      " [ 0.0891]\n",
      " [ 0.1084]\n",
      " [ 0.5315]\n",
      " [ 0.2211]\n",
      " [ 0.1083]\n",
      " [ 0.0109]\n",
      " [ 0.0703]\n",
      " [ 0.0702]\n",
      " [ 0.0407]\n",
      " [ 0.9438]\n",
      " [ 0.0702]\n",
      " [ 0.1056]\n",
      " [ 0.0702]\n",
      " [ 0.9122]\n",
      " [ 0.9991]\n",
      " [ 0.9352]\n",
      " [ 0.9122]\n",
      " [ 0.1677]\n",
      " [ 0.1734]\n",
      " [ 0.3829]\n",
      " [ 0.9121]\n",
      " [ 0.0701]\n",
      " [ 0.9118]\n",
      " [ 0.1871]\n",
      " [ 0.943 ]\n",
      " [ 0.1767]\n",
      " [ 0.07  ]\n",
      " [ 0.6793]\n",
      " [ 0.1053]\n",
      " [ 0.912 ]\n",
      " [ 0.0667]\n",
      " [ 0.606 ]\n",
      " [ 0.1673]\n",
      " [ 0.07  ]\n",
      " [ 0.1125]\n",
      " [ 0.0439]\n",
      " [ 0.9119]\n",
      " [ 0.9116]\n",
      " [ 0.9119]\n",
      " [ 0.0911]\n",
      " [ 0.7778]\n",
      " [ 0.0699]\n",
      " [ 0.755 ]\n",
      " [ 0.0699]\n",
      " [ 0.1601]\n",
      " [ 0.0699]\n",
      " [ 0.9132]\n",
      " [ 0.0706]\n",
      " [ 0.9115]\n",
      " [ 0.0698]\n",
      " [ 0.9223]\n",
      " [ 0.0592]\n",
      " [ 0.0694]\n",
      " [ 0.0698]\n",
      " [ 0.7604]\n",
      " [ 0.0698]\n",
      " [ 0.0694]\n",
      " [ 0.0694]\n",
      " [ 0.0697]\n",
      " [ 0.1942]\n",
      " [ 0.1075]\n",
      " [ 0.9117]\n",
      " [ 0.9437]\n",
      " [ 0.5287]\n",
      " [ 0.8058]\n",
      " [ 0.044 ]\n",
      " [ 0.0678]\n",
      " [ 0.7479]\n",
      " [ 0.1739]\n",
      " [ 0.8585]\n",
      " [ 0.9351]\n",
      " [ 0.0409]\n",
      " [ 0.9119]\n",
      " [ 0.0695]\n",
      " [ 0.0692]\n",
      " [ 0.8652]\n",
      " [ 0.0695]\n",
      " [ 0.4611]\n",
      " [ 0.1044]\n",
      " [ 0.0695]\n",
      " [ 0.0695]\n",
      " [ 0.1594]\n",
      " [ 0.3071]\n",
      " [ 0.0409]\n",
      " [ 0.0695]\n",
      " [ 0.0694]\n",
      " [ 0.1629]\n",
      " [ 0.1043]\n",
      " [ 0.9111]\n",
      " [ 0.061 ]\n",
      " [ 0.0258]\n",
      " [ 0.9426]\n",
      " [ 0.999 ]\n",
      " [ 0.1042]\n",
      " [ 0.1661]\n",
      " [ 0.0146]\n",
      " [ 0.1794]\n",
      " [ 0.0693]\n",
      " [ 0.166 ]\n",
      " [ 0.0906]\n",
      " [ 0.9352]\n",
      " [ 0.0677]\n",
      " [ 0.1308]\n",
      " [ 0.7887]\n",
      " [ 0.0111]\n",
      " [ 0.0692]\n",
      " [ 0.7261]\n",
      " [ 0.9109]\n",
      " [ 0.1658]\n",
      " [ 0.0041]\n",
      " [ 0.9111]\n",
      " [ 0.0664]\n",
      " [ 0.935 ]\n",
      " [ 0.0691]\n",
      " [ 0.1039]\n",
      " [ 0.7476]\n",
      " [ 0.1726]\n",
      " [ 0.0227]\n",
      " [ 0.9436]\n",
      " [ 0.9107]\n",
      " [ 0.0691]\n",
      " [ 0.0676]\n",
      " [ 0.041 ]\n",
      " [ 0.0676]\n",
      " [ 0.0548]\n",
      " [ 0.8415]\n",
      " [ 0.935 ]\n",
      " [ 0.1039]\n",
      " [ 0.6202]\n",
      " [ 0.8779]\n",
      " [ 0.1036]\n",
      " [ 0.146 ]\n",
      " [ 0.912 ]\n",
      " [ 0.0686]\n",
      " [ 0.7838]\n",
      " [ 0.1035]\n",
      " [ 0.914 ]\n",
      " [ 0.0224]\n",
      " [ 0.0094]\n",
      " [ 0.1034]\n",
      " [ 0.0596]\n",
      " [ 0.1651]\n",
      " [ 0.3342]\n",
      " [ 0.1026]\n",
      " [ 0.1159]\n",
      " [ 0.0688]\n",
      " [ 0.1482]\n",
      " [ 0.9104]\n",
      " [ 0.1033]\n",
      " [ 0.9104]\n",
      " [ 0.9106]\n",
      " [ 0.2777]\n",
      " [ 0.1035]\n",
      " [ 0.8228]\n",
      " [ 0.1032]\n",
      " [ 0.1711]\n",
      " [ 0.9106]\n",
      " [ 0.1032]\n",
      " [ 0.9436]\n",
      " [ 0.0687]\n",
      " [ 0.0687]\n",
      " [ 0.0021]\n",
      " [ 0.1031]\n",
      " [ 0.8563]\n",
      " [ 0.0503]\n",
      " [ 0.1646]\n",
      " [ 0.9104]\n",
      " [ 0.5811]\n",
      " [ 0.8772]\n",
      " [ 0.0686]\n",
      " [ 0.8706]\n",
      " [ 0.0685]\n",
      " [ 0.9349]\n",
      " [ 0.0685]\n",
      " [ 0.9435]\n",
      " [ 0.7908]\n",
      " [ 0.0685]\n",
      " [ 0.9103]\n",
      " [ 0.1507]\n",
      " [ 0.1028]\n",
      " [ 0.1028]\n",
      " [ 0.9425]\n",
      " [ 0.0412]\n",
      " [ 0.0683]\n",
      " [ 0.105 ]\n",
      " [ 0.0684]\n",
      " [ 0.0925]\n",
      " [ 0.0674]\n",
      " [ 0.8227]\n",
      " [ 0.9203]\n",
      " [ 0.9435]\n",
      " [ 0.8418]\n",
      " [ 0.138 ]\n",
      " [ 0.0684]\n",
      " [ 0.047 ]\n",
      " [ 0.1026]\n",
      " [ 0.9348]\n",
      " [ 0.0899]\n",
      " [ 0.8555]\n",
      " [ 0.7935]\n",
      " [ 0.7593]\n",
      " [ 0.0683]\n",
      " [ 0.6141]\n",
      " [ 0.0683]\n",
      " [ 0.161 ]\n",
      " [ 0.0683]\n",
      " [ 0.0682]\n",
      " [ 0.0683]\n",
      " [ 0.9348]\n",
      " [ 0.0683]\n",
      " [ 0.249 ]\n",
      " [ 0.0683]\n",
      " [ 0.8419]\n",
      " [ 0.7634]\n",
      " [ 0.1055]\n",
      " [ 0.0683]\n",
      " [ 0.1636]\n",
      " [ 0.0683]\n",
      " [ 0.9095]\n",
      " [ 0.0683]\n",
      " [ 0.1718]\n",
      " [ 0.0681]\n",
      " [ 0.8168]\n",
      " [ 0.7719]\n",
      " [ 0.0672]\n",
      " [ 0.8549]\n",
      " [ 0.1022]\n",
      " [ 0.0601]\n",
      " [ 0.0601]\n",
      " [ 0.1021]\n",
      " [ 0.9094]\n",
      " [ 0.0045]\n",
      " [ 0.9095]\n",
      " [ 0.775 ]\n",
      " [ 0.7636]\n",
      " [ 0.0682]\n",
      " [ 0.0021]\n",
      " [ 0.1037]\n",
      " [ 0.0672]\n",
      " [ 0.0682]\n",
      " [ 0.1632]\n",
      " [ 0.9094]\n",
      " [ 0.0672]\n",
      " [ 0.148 ]\n",
      " [ 0.0682]\n",
      " [ 0.0682]\n",
      " [ 0.7045]\n",
      " [ 0.0243]\n",
      " [ 0.163 ]\n",
      " [ 0.0682]\n",
      " [ 0.0682]\n",
      " [ 0.1051]\n",
      " [ 0.1433]\n",
      " [ 0.0682]\n",
      " [ 0.9092]\n",
      " [ 0.8753]\n",
      " [ 0.0854]\n",
      " [ 0.2518]\n",
      " [ 0.1355]\n",
      " [ 0.7906]\n",
      " [ 0.0682]\n",
      " [ 0.0671]\n",
      " [ 0.0682]\n",
      " [ 0.9091]\n",
      " [ 0.7329]\n",
      " [ 0.9091]\n",
      " [ 0.6145]\n",
      " [ 0.1017]\n",
      " [ 0.0682]\n",
      " [ 0.0266]\n",
      " [ 0.0681]\n",
      " [ 0.0671]\n",
      " [ 0.1016]\n",
      " [ 0.1626]\n",
      " [ 0.7274]\n",
      " [ 0.0021]\n",
      " [ 0.5334]\n",
      " [ 0.1563]\n",
      " [ 0.0604]\n",
      " [ 0.1015]\n",
      " [ 0.8422]\n",
      " [ 0.1562]\n",
      " [ 0.067 ]\n",
      " [ 0.7747]\n",
      " [ 0.0681]\n",
      " [ 0.1624]\n",
      " [ 0.1014]\n",
      " [ 0.0913]\n",
      " [ 0.1014]\n",
      " [ 0.067 ]\n",
      " [ 0.1014]\n",
      " [ 0.0681]\n",
      " [ 0.0641]\n",
      " [ 0.672 ]\n",
      " [ 0.0597]\n",
      " [ 0.9085]\n",
      " [ 0.1013]\n",
      " [ 0.9097]\n",
      " [ 0.1013]\n",
      " [ 0.9346]\n",
      " [ 0.935 ]\n",
      " [ 0.1012]\n",
      " [ 0.1012]\n",
      " [ 0.0629]\n",
      " [ 0.7382]\n",
      " [ 0.162 ]\n",
      " [ 0.7104]\n",
      " [ 0.0681]\n",
      " [ 0.0678]\n",
      " [ 0.769 ]\n",
      " [ 0.2388]\n",
      " [ 0.8403]\n",
      " [ 0.9346]\n",
      " [ 0.0681]\n",
      " [ 0.9194]\n",
      " [ 0.1648]\n",
      " [ 0.0449]\n",
      " [ 0.9141]\n",
      " [ 0.9339]\n",
      " [ 0.1048]\n",
      " [ 0.0607]\n",
      " [ 0.9425]\n",
      " [ 0.1617]\n",
      " [ 0.1009]\n",
      " [ 0.8828]\n",
      " [ 0.9433]\n",
      " [ 0.459 ]\n",
      " [ 0.1009]\n",
      " [ 0.1616]\n",
      " [ 0.011 ]\n",
      " [ 0.0677]\n",
      " [ 0.0677]\n",
      " [ 0.908 ]\n",
      " [ 0.7682]\n",
      " [ 0.1008]\n",
      " [ 0.6914]\n",
      " [ 0.068 ]\n",
      " [ 0.1008]\n",
      " [ 0.0677]\n",
      " [ 0.0153]\n",
      " [ 0.1614]\n",
      " [ 0.9336]\n",
      " [ 0.0913]\n",
      " [ 0.1007]\n",
      " [ 0.0153]\n",
      " [ 0.9089]\n",
      " [ 0.0677]\n",
      " [ 0.8788]\n",
      " [ 0.0679]\n",
      " [ 0.0677]\n",
      " [ 0.7367]\n",
      " [ 0.0608]\n",
      " [ 0.9339]\n",
      " [ 0.1612]\n",
      " [ 0.1057]\n",
      " [ 0.1047]\n",
      " [ 0.0609]\n",
      " [ 0.1374]\n",
      " [ 0.9078]\n",
      " [ 0.0045]\n",
      " [ 0.9077]\n",
      " [ 0.9088]\n",
      " [ 0.9075]\n",
      " [ 0.0679]\n",
      " [ 0.9424]\n",
      " [ 0.004 ]\n",
      " [ 0.0679]\n",
      " [ 0.0654]]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = preds > 0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418,)\n"
     ]
    }
   ],
   "source": [
    "print(preds[:,0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds =preds.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 1)\n"
     ]
    }
   ],
   "source": [
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " idx = pd.read_csv(path+'test.csv', keep_default_na=False).values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418,)\n"
     ]
    }
   ],
   "source": [
    "print(idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = np.stack([idx,preds[:,0]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[892 0]\n",
      " [893 1]\n",
      " [894 0]\n",
      " [895 0]\n",
      " [896 1]\n",
      " [897 0]\n",
      " [898 1]\n",
      " [899 0]\n",
      " [900 1]\n",
      " [901 0]\n",
      " [902 0]\n",
      " [903 0]\n",
      " [904 1]\n",
      " [905 0]\n",
      " [906 1]\n",
      " [907 1]\n",
      " [908 0]\n",
      " [909 0]\n",
      " [910 1]\n",
      " [911 1]\n",
      " [912 0]\n",
      " [913 0]\n",
      " [914 1]\n",
      " [915 0]\n",
      " [916 1]\n",
      " [917 0]\n",
      " [918 1]\n",
      " [919 0]\n",
      " [920 0]\n",
      " [921 0]\n",
      " [922 0]\n",
      " [923 0]\n",
      " [924 1]\n",
      " [925 1]\n",
      " [926 0]\n",
      " [927 0]\n",
      " [928 1]\n",
      " [929 1]\n",
      " [930 0]\n",
      " [931 0]\n",
      " [932 0]\n",
      " [933 0]\n",
      " [934 0]\n",
      " [935 1]\n",
      " [936 1]\n",
      " [937 0]\n",
      " [938 0]\n",
      " [939 0]\n",
      " [940 1]\n",
      " [941 1]\n",
      " [942 0]\n",
      " [943 0]\n",
      " [944 0]\n",
      " [945 0]\n",
      " [946 0]\n",
      " [947 0]\n",
      " [948 0]\n",
      " [949 0]\n",
      " [950 0]\n",
      " [951 1]\n",
      " [952 0]\n",
      " [953 0]\n",
      " [954 0]\n",
      " [955 1]\n",
      " [956 1]\n",
      " [957 1]\n",
      " [958 1]\n",
      " [959 0]\n",
      " [960 0]\n",
      " [961 0]\n",
      " [962 1]\n",
      " [963 0]\n",
      " [964 1]\n",
      " [965 0]\n",
      " [966 1]\n",
      " [967 0]\n",
      " [968 0]\n",
      " [969 1]\n",
      " [970 0]\n",
      " [971 1]\n",
      " [972 0]\n",
      " [973 1]\n",
      " [974 0]\n",
      " [975 0]\n",
      " [976 0]\n",
      " [977 0]\n",
      " [978 1]\n",
      " [979 1]\n",
      " [980 1]\n",
      " [981 0]\n",
      " [982 1]\n",
      " [983 0]\n",
      " [984 1]\n",
      " [985 0]\n",
      " [986 0]\n",
      " [987 0]\n",
      " [988 1]\n",
      " [989 0]\n",
      " [990 1]\n",
      " [991 0]\n",
      " [992 1]\n",
      " [993 0]\n",
      " [994 0]\n",
      " [995 0]\n",
      " [996 1]\n",
      " [997 0]\n",
      " [998 0]\n",
      " [999 0]\n",
      " [1000 0]\n",
      " [1001 0]\n",
      " [1002 0]\n",
      " [1003 1]\n",
      " [1004 1]\n",
      " [1005 0]\n",
      " [1006 1]\n",
      " [1007 0]\n",
      " [1008 0]\n",
      " [1009 1]\n",
      " [1010 0]\n",
      " [1011 1]\n",
      " [1012 1]\n",
      " [1013 0]\n",
      " [1014 1]\n",
      " [1015 0]\n",
      " [1016 0]\n",
      " [1017 1]\n",
      " [1018 0]\n",
      " [1019 0]\n",
      " [1020 0]\n",
      " [1021 0]\n",
      " [1022 0]\n",
      " [1023 0]\n",
      " [1024 0]\n",
      " [1025 0]\n",
      " [1026 0]\n",
      " [1027 0]\n",
      " [1028 0]\n",
      " [1029 0]\n",
      " [1030 1]\n",
      " [1031 0]\n",
      " [1032 0]\n",
      " [1033 1]\n",
      " [1034 1]\n",
      " [1035 0]\n",
      " [1036 0]\n",
      " [1037 0]\n",
      " [1038 0]\n",
      " [1039 0]\n",
      " [1040 0]\n",
      " [1041 0]\n",
      " [1042 1]\n",
      " [1043 0]\n",
      " [1044 0]\n",
      " [1045 1]\n",
      " [1046 0]\n",
      " [1047 0]\n",
      " [1048 1]\n",
      " [1049 1]\n",
      " [1050 0]\n",
      " [1051 0]\n",
      " [1052 1]\n",
      " [1053 0]\n",
      " [1054 1]\n",
      " [1055 0]\n",
      " [1056 0]\n",
      " [1057 1]\n",
      " [1058 0]\n",
      " [1059 0]\n",
      " [1060 1]\n",
      " [1061 1]\n",
      " [1062 0]\n",
      " [1063 0]\n",
      " [1064 0]\n",
      " [1065 0]\n",
      " [1066 0]\n",
      " [1067 1]\n",
      " [1068 1]\n",
      " [1069 0]\n",
      " [1070 1]\n",
      " [1071 1]\n",
      " [1072 0]\n",
      " [1073 0]\n",
      " [1074 1]\n",
      " [1075 0]\n",
      " [1076 1]\n",
      " [1077 0]\n",
      " [1078 1]\n",
      " [1079 0]\n",
      " [1080 0]\n",
      " [1081 0]\n",
      " [1082 0]\n",
      " [1083 0]\n",
      " [1084 0]\n",
      " [1085 0]\n",
      " [1086 0]\n",
      " [1087 0]\n",
      " [1088 0]\n",
      " [1089 1]\n",
      " [1090 0]\n",
      " [1091 1]\n",
      " [1092 1]\n",
      " [1093 0]\n",
      " [1094 0]\n",
      " [1095 1]\n",
      " [1096 0]\n",
      " [1097 0]\n",
      " [1098 1]\n",
      " [1099 0]\n",
      " [1100 1]\n",
      " [1101 0]\n",
      " [1102 0]\n",
      " [1103 0]\n",
      " [1104 0]\n",
      " [1105 1]\n",
      " [1106 0]\n",
      " [1107 0]\n",
      " [1108 1]\n",
      " [1109 0]\n",
      " [1110 1]\n",
      " [1111 0]\n",
      " [1112 1]\n",
      " [1113 0]\n",
      " [1114 1]\n",
      " [1115 0]\n",
      " [1116 1]\n",
      " [1117 1]\n",
      " [1118 0]\n",
      " [1119 1]\n",
      " [1120 0]\n",
      " [1121 0]\n",
      " [1122 0]\n",
      " [1123 1]\n",
      " [1124 0]\n",
      " [1125 0]\n",
      " [1126 0]\n",
      " [1127 0]\n",
      " [1128 0]\n",
      " [1129 0]\n",
      " [1130 1]\n",
      " [1131 1]\n",
      " [1132 1]\n",
      " [1133 1]\n",
      " [1134 0]\n",
      " [1135 0]\n",
      " [1136 0]\n",
      " [1137 0]\n",
      " [1138 1]\n",
      " [1139 0]\n",
      " [1140 1]\n",
      " [1141 1]\n",
      " [1142 1]\n",
      " [1143 0]\n",
      " [1144 1]\n",
      " [1145 0]\n",
      " [1146 0]\n",
      " [1147 0]\n",
      " [1148 0]\n",
      " [1149 0]\n",
      " [1150 1]\n",
      " [1151 0]\n",
      " [1152 0]\n",
      " [1153 0]\n",
      " [1154 1]\n",
      " [1155 1]\n",
      " [1156 0]\n",
      " [1157 0]\n",
      " [1158 0]\n",
      " [1159 0]\n",
      " [1160 1]\n",
      " [1161 0]\n",
      " [1162 0]\n",
      " [1163 0]\n",
      " [1164 1]\n",
      " [1165 1]\n",
      " [1166 0]\n",
      " [1167 1]\n",
      " [1168 0]\n",
      " [1169 0]\n",
      " [1170 0]\n",
      " [1171 0]\n",
      " [1172 1]\n",
      " [1173 0]\n",
      " [1174 1]\n",
      " [1175 1]\n",
      " [1176 1]\n",
      " [1177 0]\n",
      " [1178 0]\n",
      " [1179 0]\n",
      " [1180 0]\n",
      " [1181 0]\n",
      " [1182 0]\n",
      " [1183 1]\n",
      " [1184 0]\n",
      " [1185 0]\n",
      " [1186 0]\n",
      " [1187 0]\n",
      " [1188 1]\n",
      " [1189 0]\n",
      " [1190 0]\n",
      " [1191 0]\n",
      " [1192 0]\n",
      " [1193 0]\n",
      " [1194 0]\n",
      " [1195 0]\n",
      " [1196 1]\n",
      " [1197 1]\n",
      " [1198 0]\n",
      " [1199 0]\n",
      " [1200 0]\n",
      " [1201 1]\n",
      " [1202 0]\n",
      " [1203 0]\n",
      " [1204 0]\n",
      " [1205 1]\n",
      " [1206 1]\n",
      " [1207 1]\n",
      " [1208 1]\n",
      " [1209 0]\n",
      " [1210 0]\n",
      " [1211 0]\n",
      " [1212 0]\n",
      " [1213 0]\n",
      " [1214 0]\n",
      " [1215 0]\n",
      " [1216 1]\n",
      " [1217 0]\n",
      " [1218 0]\n",
      " [1219 0]\n",
      " [1220 0]\n",
      " [1221 0]\n",
      " [1222 1]\n",
      " [1223 0]\n",
      " [1224 0]\n",
      " [1225 1]\n",
      " [1226 0]\n",
      " [1227 0]\n",
      " [1228 0]\n",
      " [1229 0]\n",
      " [1230 0]\n",
      " [1231 0]\n",
      " [1232 0]\n",
      " [1233 0]\n",
      " [1234 0]\n",
      " [1235 1]\n",
      " [1236 0]\n",
      " [1237 1]\n",
      " [1238 0]\n",
      " [1239 1]\n",
      " [1240 0]\n",
      " [1241 1]\n",
      " [1242 1]\n",
      " [1243 0]\n",
      " [1244 0]\n",
      " [1245 0]\n",
      " [1246 1]\n",
      " [1247 0]\n",
      " [1248 1]\n",
      " [1249 0]\n",
      " [1250 0]\n",
      " [1251 1]\n",
      " [1252 0]\n",
      " [1253 1]\n",
      " [1254 1]\n",
      " [1255 0]\n",
      " [1256 1]\n",
      " [1257 0]\n",
      " [1258 0]\n",
      " [1259 1]\n",
      " [1260 1]\n",
      " [1261 0]\n",
      " [1262 0]\n",
      " [1263 1]\n",
      " [1264 0]\n",
      " [1265 0]\n",
      " [1266 1]\n",
      " [1267 1]\n",
      " [1268 0]\n",
      " [1269 0]\n",
      " [1270 0]\n",
      " [1271 0]\n",
      " [1272 0]\n",
      " [1273 0]\n",
      " [1274 1]\n",
      " [1275 1]\n",
      " [1276 0]\n",
      " [1277 1]\n",
      " [1278 0]\n",
      " [1279 0]\n",
      " [1280 0]\n",
      " [1281 0]\n",
      " [1282 0]\n",
      " [1283 1]\n",
      " [1284 0]\n",
      " [1285 0]\n",
      " [1286 0]\n",
      " [1287 1]\n",
      " [1288 0]\n",
      " [1289 1]\n",
      " [1290 0]\n",
      " [1291 0]\n",
      " [1292 1]\n",
      " [1293 0]\n",
      " [1294 1]\n",
      " [1295 0]\n",
      " [1296 0]\n",
      " [1297 0]\n",
      " [1298 0]\n",
      " [1299 0]\n",
      " [1300 1]\n",
      " [1301 0]\n",
      " [1302 1]\n",
      " [1303 1]\n",
      " [1304 1]\n",
      " [1305 0]\n",
      " [1306 1]\n",
      " [1307 0]\n",
      " [1308 0]\n",
      " [1309 0]]\n"
     ]
    }
   ],
   "source": [
    "print(subm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_file_name = 'submission4.csv'\n",
    "np.savetxt(submission_file_name, subm, fmt='%d,%d', header='PassengerId,Survived', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='submission4.csv' target='_blank'>submission4.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/nbs/submission4.csv"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink(submission_file_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
